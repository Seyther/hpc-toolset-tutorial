## Acknowledgments

Thank you to the staff of the [Ohio Supercomputer Center](https://osc.edu), [University at Buffalo Center for Computational Research](https://buffalo.edu/ccr), and [Virginia Tech Advanced Research Computing](https://arc.vt.edu/) for developing this tutorial.  

#### Funding
Thank you to the [National Science Foundation](https://nsf.gov) for the supporting the development of [Open OnDemand](https://openondemand.org) and [Open XDMoD](https://open.xdmod.org/)  
- Open OnDemand NSF award numbers: [NSF#1534949](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1534949) and [NSF#1935725](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1835725)  

- XDMoD NSF award numbers: [ACI 1025159](https://nsf.gov/awardsearch/showAward?AWD_ID=1025159), [ACI 1445806](https://nsf.gov/awardsearch/showAward?AWD_ID=1445806), and [OAC 2137603](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2137603)  

#### Publications
- Andrew E. Bruno, Doris J. Sajdak. 2021. ColdFront: Resource Allocation Management System (PEARC ’21). Association for Computing Machinery, New York, NY, USA. DOI:[10.1145/3437359.3465585](https://doi.org/10.1145/3437359.3465585)  

- Dave Hudak et al., (2018). Open OnDemand: A web-based client portal for HPC centers. Journal of Open Source Software, 3(25), 622, https://doi.org/10.21105/joss.00622  

- Jeffrey T. Palmer, Steven M. Gallo, Thomas R. Furlani, Matthew D. Jones, Robert L. DeLeon, Joseph P. White, Nikolay Simakov, Abani K. Patra, Jeanette Sperhac, Thomas Yearke, Ryan Rathsam, Martins Innus, Cynthia D. Cornelius, James C. Browne, William L. Barth, Richard T. Evans, “Open XDMoD: A Tool for the Comprehensive Management of High-Performance Computing Resources”, Computing in Science & Engineering, Vol 17, Issue 4, 2015, pp. 52-62. [10.1109/MCSE.2015.68](http://dx.doi.org/10.1109/MCSE.2015.68)  

#### Workshops
This tutorial was first presented at the PRACTICE AND EXPERIENCE IN ADVANCED RESEARCH COMPUTING 2020 Virtual Conference (PEARC20) - https://pearc.acm.org/pearc20/  

Since then this tutorial has been updated and presented, in full or in part, at the following conferences:

[Global OnDemand 2025](https://www.conference2025.openondemand.org/)  
[Gateways 2024](https://sciencegateways.org/gateways2024-program)  
[Gateways 2023](https://sciencegateways.org/gateways2023-program)  
[PEARC23](https://pearc.acm.org/pearc23/)  
[ISC23](https://www.isc-hpc.com/)  
[Gateways 2022](https://sciencegateways.org/gateways2022-program)  
[PEARC22](https://pearc.acm.org/pearc22)  
[Gateways 2021](https://sciencegateways.org/gateways2021-program)  
[PEARC21](https://pearc.acm.org/pearc21)  
[Gateways 2020](https://sciencegateways.org/web/gateways2020)  

#### Container Development

The multi-container Slurm cluster using docker-compose is loosely based on the following:

- https://github.com/giovtorres/slurm-docker-cluster
- https://github.com/OSC/ood-images/tree/master/docker-with-slurm


[Back to Start](../README.md)
